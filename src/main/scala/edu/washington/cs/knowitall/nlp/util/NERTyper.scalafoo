package edu.washington.cs.knowitall.nlp.util

import scala.collection.JavaConversions._
import opennlp.tools.tokenize.{TokenizerME, TokenizerModel}
import edu.washington.cs.knowitall.util.DefaultObjects
import collection.mutable.ArrayBuffer
import opennlp.tools.util.Span
import edu.washington.cs.knowitall.browser.hadoop.scoobi.util.ExtractionSentenceRecord
import java.util.ArrayList

import edu.washington.cs.knowitall.commonlib.Range
import edu.washington.cs.knowitall.`type`.tag.StanfordNamedEntityTagger
import edu.washington.cs.knowitall.Sentence
import edu.washington.cs.knowitall.nlp.ChunkedSentence
import java.util
import util.regex.Pattern


/**
 * Created with IntelliJ IDEA.
 * User: niranjan
 * Date: 8/29/12
 * Time: 10:28 AM
 * To change this template use File | Settings | File Templates.
 */

object NERTyper {

  val tokenizerModel = new TokenizerModel(
    DefaultObjects.getResourceAsStream(DefaultObjects.tokenizerModelFile))
  val threadLocal = new ThreadLocal[TokenizerME] {
    override def initialValue() = {
      new TokenizerME(tokenizerModel);
    }
  };

  def getTokenizer = threadLocal.get
}
class NERTyper {

  val netagger = new StanfordNamedEntityTagger("netagger")


  import NERTyper._
  def chunkedSentence(text:String, postags:Seq[String], chunktags:Seq[String]):ChunkedSentence = {

    var out:ChunkedSentence = null
    val tokenizer = getTokenizer
    try {
      val offsets  = tokenizer.tokenizePos(text)
      val ranges = new ArrayBuffer[Range](offsets.length)
      val tokenList = new ArrayBuffer[String](offsets.length)
      for (span:Span <- offsets) {
        ranges.add(Range.fromInterval(span.getStart(), span.getEnd()))
        tokenList.add(text.substring(span.getStart(), span.getEnd()))
      }
      out = new ChunkedSentence(ranges.toArray, tokenList.toArray, postags.toArray, chunktags.toArray)
    }catch {
      case e:Exception => {
        println("Caught exception building chunked sentence: " + text + " postags: " + postags)
        e.printStackTrace()
        out = new ChunkedSentence(new ArrayBuffer[String].toArray, new ArrayBuffer[String].toArray, new ArrayBuffer[String].toArray)
      }

    }
    //}
    return out
  }

  def getTags(record:ExtractionSentenceRecord):Seq[Type] = {

    try {
      val sentence = new Sentence(chunkedSentence(record.sentence, record.posTags, record.chunkTags), record.sentence)
      netagger.findTags(sentence)//, tokenize(record.text), record.lemmas.split(" "), record.postags.split(" "), record.chunktags.split(" ")))//record.chunkedSentence, record.text))
    }catch {
      case e:Exception => {
        println("Caught NETagger exception/error for record: " + record.sentence)
        new ArrayList[Type]
      }
      case e:Error => {
        println("caught NETTagger error for record: " + record.sentence)
        new ArrayList[Type]()
      }
      case _ => {
        println("Caught NERTagger exception/error for record: " + record.sentence)
        new ArrayList[Type]
      }
    }
  }

  val fullTagRe = """^%s$"""
  val startTagRe = """^%s\s"""
  val endTagRe = """\s%s$"""

  def argumentMatchesTagText(tag: Type, argument: String): Boolean = {
    val quotedTagText = Pattern.quote(tag.text)
    return fullTagRe.format(quotedTagText).r.findFirstIn(argument) != None || startTagRe.format(quotedTagText).r.findFirstIn(argument) != None || endTagRe.format(quotedTagText).r.findFirstIn(argument) != None
  }

  def toTypes(arg:String, tags:Iterable[Type]): Seq[Type] = {
    tags.filter(tag => argumentMatchesTagText(tag, arg)).map(tag => new Type(tag.descriptor, tag.text, "NER", 1.0)).toSeq
  }


  def assignTypes(er:ExtractionSentenceRecord): (Seq[Type], Seq[Type]) = {
    val tags = getTags(er)
    (toTypes(er.arg1, tags), toTypes(er.arg2, tags))
  }

  def assignTypes(er:ExtractionSentenceRecord, argument:String): Seq[Type] = {
    return getTags(er).filter(tag => argumentMatchesTagText(tag, argument))
                      .map(tag => new Type(tag.descriptor, tag.text, "NER", 1.0))
  }

  def assignTypes(argument: String, context: Any): Seq[Type] = {
    val er = new ExtractionSentenceRecord(context.toString.split("\t"))
    return assignTypes(er, argument)
  }

}